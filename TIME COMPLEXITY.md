# Time Complexity in Data Structures and Algorithms (DSA)

Time complexity tells us how fast or slow an algorithm runs as the input size increases. Understanding time complexity helps you choose or design efficient algorithms for your problems.

Below is a list of **common time complexities** in **increasing order (fastest to slowest)**:

---

## âœ… 1. O(1) â€” Constant Time
- Time doesn't change with input size.
- ðŸ“Œ **Example:** Accessing an array element `arr[5]`

---

## âœ… 2. O(log n) â€” Logarithmic Time
- Time grows slowly as input size increases.
- ðŸ“Œ **Example:** Binary Search on a sorted array

---

## âœ… 3. O(âˆšn) â€” Square Root Time
- Time grows with the square root of the input size.
- ðŸ“Œ **Example:**
  - Prime number check up to âˆšn
  - Jump Search
  - Factor counting

---

## âœ… 4. O(n) â€” Linear Time
- Time increases proportionally with input.
- ðŸ“Œ **Example:** Traversing an array of size `n`

---

## âœ… 5. O(n log n) â€” Linearithmic Time
- Slightly more than linear but better than quadratic.
- ðŸ“Œ **Example:** Merge Sort, Heap Sort

---

## âœ… 6. O(nÂ²) â€” Quadratic Time
- Time increases as the square of input.
- ðŸ“Œ **Example:** Bubble Sort, Selection Sort

---

## âœ… 7. O(nÂ³) â€” Cubic Time
- Time increases as the cube of input.
- ðŸ“Œ **Example:** 3 nested loops (e.g., matrix multiplication)

---

## âœ… 8. O(2^n) â€” Exponential Time
- Time doubles with each additional input unit.
- ðŸ“Œ **Example:** Recursive Fibonacci (without memoization)

---

## âœ… 9. O(n!) â€” Factorial Time
- One of the slowest time complexities.
- ðŸ“Œ **Example:** Solving the Traveling Salesman Problem with brute force

---

## â†’ **Summary: Time Complexities in Increasing Order**
```
O(1) < O(log n) < O(âˆšn) < O(n) < O(n log n) < O(nÂ²) < O(nÂ³) < O(2^n) < O(n!)
```

Use this cheat sheet whenever you need to analyze or compare the efficiency of algorithms. It helps in both competitive programming and real-world problem solving!

